{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2 Clickstream Data to Iceberg\n",
    "\n",
    "This notebook loads clickstream data from R2 for a specific day and hour, then loads it into an Iceberg table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import sys\n",
    "import json\n",
    "# Install Boto3 if not already installed\n",
    "!{sys.executable} -m pip install boto3\n",
    "import boto3\n",
    "# Install duckdb if not already installed\n",
    "!{sys.executable} -m pip install duckdb\n",
    "import duckdb\n",
    "# Install PyIceberg if not already installed\n",
    "!{sys.executable} -m pip install pyiceberg\n",
    "from pyiceberg.catalog.rest import RestCatalog\n",
    "from pyiceberg.exceptions import NamespaceAlreadyExistsError\n",
    "\n",
    "# Import our UUID helper functions\n",
    "sys.path.append(os.path.join(os.getcwd()))\n",
    "from uuid_fix import stringify_uuids, check_for_uuids, convert_uuids_in_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Environment\n",
    "\n",
    "Set up environment variables for both R2 and Iceberg catalog connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_access_key = os.environ.get(\"R2_ACCESS_KEY_ID\")\n",
    "r2_secret_key = os.environ.get(\"R2_SECRET_ACCESS_KEY\")\n",
    "r2_endpoint = os.environ.get(\"R2_ENDPOINT\")\n",
    "r2_bucket = os.environ.get(\"R2_CLICKSTREAM_BUCKET\", \"analytics-pipeline\")\n",
    "\n",
    "warehouse = os.environ.get(\"WAREHOUSE\")\n",
    "token = os.environ.get(\"TOKEN\")\n",
    "catalog_uri = os.environ.get(\"CATALOG_URI\")\n",
    "\n",
    "missing_r2 = []\n",
    "if not r2_endpoint: missing_r2.append(\"R2_ENDPOINT\")\n",
    "if not r2_access_key: missing_r2.append(\"R2_ACCESS_KEY\")\n",
    "if not r2_secret_key: missing_r2.append(\"R2_SECRET_KEY\")\n",
    "\n",
    "missing_iceberg = []\n",
    "if not warehouse: missing_iceberg.append(\"WAREHOUSE\")\n",
    "if not token: missing_iceberg.append(\"TOKEN\")\n",
    "if not catalog_uri: missing_iceberg.append(\"CATALOG_URI\")\n",
    "\n",
    "if missing_r2:\n",
    "    print(f\"Warning: Missing required R2 credentials: {', '.join(missing_r2)}\")\n",
    "if missing_iceberg:\n",
    "    print(f\"Warning: Missing required Iceberg credentials: {', '.join(missing_iceberg)}\")\n",
    "\n",
    "print(f\"R2 Bucket: {r2_bucket}\")\n",
    "print(f\"Warehouse: {warehouse}\")\n",
    "print(f\"Catalog URI: {catalog_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to R2 and Iceberg\n",
    "\n",
    "Establish connections to R2 Storage and Iceberg catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if all([r2_access_key, r2_secret_key]):\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=r2_access_key,\n",
    "            aws_secret_access_key=r2_secret_key,\n",
    "            endpoint_url=r2_endpoint\n",
    "        )\n",
    "        print(\"Connected to R2 successfully!\")\n",
    "    else:\n",
    "        print(\"Cannot connect to R2 - missing required credentials\")\n",
    "        s3_client = None\n",
    "except Exception as e:\n",
    "    print(f\"R2 connection failed: {str(e)}\")\n",
    "    s3_client = None\n",
    "\n",
    "try:\n",
    "    if all([warehouse, token, catalog_uri]):\n",
    "        catalog = RestCatalog(\n",
    "            name=\"my_catalog\",\n",
    "            warehouse=warehouse,\n",
    "            uri=catalog_uri,\n",
    "            token=token,\n",
    "        )\n",
    "        print(\"Connected to Iceberg catalog successfully!\")\n",
    "    else:\n",
    "        print(\"Cannot connect to Iceberg catalog - missing required credentials\")\n",
    "        catalog = None\n",
    "except Exception as e:\n",
    "    print(f\"Iceberg connection failed: {str(e)}\")\n",
    "    catalog = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Parameters\n",
    "\n",
    "Configure which day and hour of data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_date = \"2025-05-11\"  # Format: YYYY-MM-DD\n",
    "event_hour = \"01\"          # Format: HH (24-hour)\n",
    "\n",
    "s3_prefix = f\"event_date={event_date}/hr={event_hour}/\"\n",
    "print(f\"Will process data in: {s3_prefix}\")\n",
    "\n",
    "iceberg_namespace = \"default\"\n",
    "iceberg_table = \"clickstream_events\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Namespace (if needed)\n",
    "\n",
    "Ensure the target namespace exists in the Iceberg catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if catalog is not None:\n",
    "    try:\n",
    "        catalog.create_namespace(iceberg_namespace)\n",
    "        print(f\"Created '{iceberg_namespace}' namespace\")\n",
    "    except NamespaceAlreadyExistsError:\n",
    "        print(f\"'{iceberg_namespace}' namespace already exists\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating namespace: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available S3 Objects\n",
    "\n",
    "Check what data files are available for the selected day and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_objects(bucket, prefix):\n",
    "    if s3_client is None:\n",
    "        return \"Not connected to R2\"\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        if 'Contents' in response:\n",
    "            return [obj['Key'] for obj in response['Contents']]\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        return f\"Error listing objects: {str(e)}\"\n",
    "\n",
    "s3_objects = list_s3_objects(r2_bucket, s3_prefix)\n",
    "\n",
    "if isinstance(s3_objects, list):\n",
    "    print(f\"Found {len(s3_objects)} objects in the specified path\")\n",
    "    if s3_objects:\n",
    "        for i, obj in enumerate(s3_objects[:5]):\n",
    "            print(f\"- {obj}\")\n",
    "        if len(s3_objects) > 5:\n",
    "            print(f\"... and {len(s3_objects) - 5} more\")\n",
    "    else:\n",
    "        print(\"No data files found for the specified date and hour\")\n",
    "else:\n",
    "    print(s3_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from R2\n",
    "\n",
    "Load the clickstream data from R2 into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file = \"/home/jovyan/duckdb_data/clickstream.db\"\n",
    "os.makedirs(os.path.dirname(db_file), exist_ok=True)\n",
    "\n",
    "con = duckdb.connect(db_file)\n",
    "\n",
    "# Optional: Set some pragmas for better performance\n",
    "con.execute(\"PRAGMA temp_directory='/home/jovyan/duckdb_data/temp'\")\n",
    "con.execute(\"PRAGMA memory_limit='100GB'\")\n",
    "\n",
    "def read_s3_with_duckdb(bucket, key):\n",
    "    try:\n",
    "        con.execute(f\"\"\"\n",
    "            SET s3_region='auto';\n",
    "            SET s3_endpoint='{r2_endpoint.replace('https://', '')}';\n",
    "            SET s3_access_key_id='{r2_access_key}';\n",
    "            SET s3_secret_access_key='{r2_secret_key}';\n",
    "        \"\"\")\n",
    "        \n",
    "        s3_url = f\"s3://{bucket}/{key}\"\n",
    "        print(f\"Reading {s3_url}\")\n",
    "        \n",
    "        if key.endswith('.json.gz'):\n",
    "            query = f\"\"\"\n",
    "                SELECT * FROM read_json_auto('{s3_url}', \n",
    "                    filename=true, \n",
    "                    maximum_object_size=3000000000,\n",
    "                    ignore_errors=true)\n",
    "            \"\"\"\n",
    "        elif key.endswith('.parquet'):\n",
    "            query = f\"SELECT * FROM read_parquet('{s3_url}')\"\n",
    "        elif key.endswith('.csv') or key.endswith('.tsv') or key.endswith('.txt'):\n",
    "            query = f\"SELECT * FROM read_csv_auto('{s3_url}')\"\n",
    "        else:\n",
    "            return None, f\"Unsupported file type: {key}\"\n",
    "        \n",
    "        result = con.execute(query).fetchdf()\n",
    "        return result, None\n",
    "    except Exception as e:\n",
    "        return None, f\"Error reading file with DuckDB: {str(e)}\"\n",
    "\n",
    "all_data = []\n",
    "error_count = 0\n",
    "valid_count = 0\n",
    "\n",
    "if isinstance(s3_objects, list) and s3_objects:\n",
    "    for obj_key in s3_objects:\n",
    "        print(f\"Processing {obj_key} with DuckDB...\")\n",
    "        \n",
    "        df, error = read_s3_with_duckdb(r2_bucket, obj_key)\n",
    "            \n",
    "        if df is not None:\n",
    "            row_count = len(df)\n",
    "            print(f\"Successfully loaded {row_count} rows from {obj_key}\")\n",
    "            all_data.append(df)\n",
    "            valid_count += 1\n",
    "        else:\n",
    "            print(f\"Failed to load {obj_key}: {error}\")\n",
    "            error_count += 1\n",
    "    \n",
    "    if all_data:\n",
    "        clickstream_data = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"Loaded {len(clickstream_data)} total rows from {valid_count} files\")\n",
    "        print(f\"Failed to load {error_count} files\")\n",
    "        \n",
    "        print(clickstream_data.head())\n",
    "    else:\n",
    "        print(\"No data was loaded successfully\")\n",
    "        clickstream_data = None\n",
    "else:\n",
    "    print(\"No data files to process\")\n",
    "    clickstream_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data for Iceberg\n",
    "\n",
    "Clean and prepare the data for Iceberg storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_clickstream_data(df):\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    if 'timestamp' in processed_df.columns and processed_df['timestamp'].dtype == 'object':\n",
    "        processed_df['timestamp'] = pd.to_datetime(processed_df['timestamp'])\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "if clickstream_data is not None:\n",
    "    processed_data = process_clickstream_data(clickstream_data)\n",
    "    if processed_data is not None:\n",
    "        print(f\"Data processed successfully: {len(processed_data)} rows\")\n",
    "        processed_data.head()\n",
    "    else:\n",
    "        print(\"Data processing failed\")\n",
    "else:\n",
    "    processed_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Iceberg Table\n",
    "\n",
    "Create a new Iceberg table if it doesn't exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_pyarrow(df):\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df_copy = convert_uuids_in_dataframe(df)\n",
    "        \n",
    "        if 'timestamp' in df_copy.columns and pd.api.types.is_datetime64_any_dtype(df_copy['timestamp']):\n",
    "            print(\"Converting timestamp to ISO format string\")\n",
    "            if df_copy['timestamp'].dt.tz is not None:\n",
    "                df_copy['timestamp'] = df_copy['timestamp'].dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "            df_copy['timestamp'] = df_copy['timestamp'].dt.strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
    "        \n",
    "        if 'event_date' in df_copy.columns:\n",
    "            if pd.api.types.is_datetime64_any_dtype(df_copy['event_date']):\n",
    "                print(\"Converting event_date datetime to string format\")\n",
    "                df_copy['event_date'] = df_copy['event_date'].dt.strftime('%Y-%m-%d')\n",
    "            else:\n",
    "                print(\"event_date is not a datetime, keeping as is\")\n",
    "        \n",
    "        for col in ['session_data', 'device_info', 'event_data', 'raw_event']:\n",
    "            if col in df_copy.columns:\n",
    "                print(f\"Converting column {col} to JSON strings\")\n",
    "                df_copy[col] = df_copy[col].apply(\n",
    "                    lambda x: json.dumps(x) if isinstance(x, (dict, list)) else \n",
    "                             (str(x) if not isinstance(x, (str, int, float, bool, type(None))) else x)\n",
    "                )\n",
    "        \n",
    "        return pa.Table.from_pandas(df_copy, preserve_index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting to PyArrow: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def create_iceberg_table(catalog, namespace, table_name, schema):\n",
    "    if catalog is None:\n",
    "        return \"Not connected to Iceberg catalog\"\n",
    "    \n",
    "    try:\n",
    "        table_identifier = (namespace, table_name)\n",
    "        if not catalog.table_exists(table_identifier):\n",
    "            table = catalog.create_table(\n",
    "                table_identifier,\n",
    "                schema=schema,\n",
    "                partition_spec=[\"event_date\", \"hr\"]\n",
    "            )\n",
    "            return f\"Created table: {table_name}\"\n",
    "        else:\n",
    "            table = catalog.load_table(table_identifier)\n",
    "            return f\"Table already exists: {table_name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error creating/loading table: {str(e)}\"\n",
    "\n",
    "if processed_data is not None:\n",
    "    arrow_table = df_to_pyarrow(processed_data)\n",
    "    if arrow_table is not None:\n",
    "        print(\"Data converted to PyArrow Table successfully\")\n",
    "        print(f\"Schema: {arrow_table.schema}\")\n",
    "        \n",
    "        table_result = create_iceberg_table(catalog, iceberg_namespace, iceberg_table, arrow_table.schema)\n",
    "        print(table_result)\n",
    "    else:\n",
    "        print(\"Failed to convert data to PyArrow Table\")\n",
    "else:\n",
    "    print(\"No processed data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data to Iceberg Table\n",
    "\n",
    "Write the processed clickstream data to the Iceberg table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_iceberg_table(catalog, namespace, table_name, data):\n",
    "    if catalog is None:\n",
    "        return \"Not connected to Iceberg catalog\"\n",
    "    \n",
    "    if data is None:\n",
    "        return \"No data to append\"\n",
    "        \n",
    "    try:\n",
    "        table_identifier = (namespace, table_name)\n",
    "        if catalog.table_exists(table_identifier):\n",
    "            table = catalog.load_table(table_identifier)\n",
    "            \n",
    "            table.append(data)\n",
    "            return f\"Data appended to table: {table_name}\"\n",
    "        else:\n",
    "            return f\"Table does not exist: {table_name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error appending data: {str(e)}\"\n",
    "\n",
    "if arrow_table is not None:\n",
    "    append_result = append_to_iceberg_table(catalog, iceberg_namespace, iceberg_table, arrow_table)\n",
    "    print(append_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data in Iceberg Table\n",
    "\n",
    "Query the Iceberg table to verify the data was written successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_iceberg_table(catalog, namespace, table_name):\n",
    "    if catalog is None:\n",
    "        return \"Not connected to Iceberg catalog\"\n",
    "        \n",
    "    try:\n",
    "        table_identifier = (namespace, table_name)\n",
    "        if catalog.table_exists(table_identifier):\n",
    "            table = catalog.load_table(table_identifier)\n",
    "            \n",
    "            scanned = table.scan().to_arrow()\n",
    "            if len(scanned) > 0:\n",
    "                return scanned.to_pandas()\n",
    "            else:\n",
    "                return \"Table exists but has no data\"\n",
    "        else:\n",
    "            return f\"Table does not exist: {table_name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error querying table: {str(e)}\"\n",
    "\n",
    "result = query_iceberg_table(catalog, iceberg_namespace, iceberg_table)\n",
    "if isinstance(result, pd.DataFrame):\n",
    "    print(f\"Retrieved {len(result)} rows from Iceberg table\")\n",
    "    result.head()\n",
    "else:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data by Partition\n",
    "\n",
    "Query the Iceberg table with partition filters to verify partitioning works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_filters(catalog, namespace, table_name, filters):\n",
    "    if catalog is None:\n",
    "        return \"Not connected to Iceberg catalog\"\n",
    "        \n",
    "    try:\n",
    "        table_identifier = (namespace, table_name)\n",
    "        if catalog.table_exists(table_identifier):\n",
    "            table = catalog.load_table(table_identifier)\n",
    "            \n",
    "            scan = table.scan()\n",
    "            for col, op, val in filters:\n",
    "                scan = scan.filter(getattr(scan, col)[op](val))\n",
    "                \n",
    "            results = scan.to_arrow()\n",
    "            if len(results) > 0:\n",
    "                return results.to_pandas()\n",
    "            else:\n",
    "                return \"No data matching filters\"\n",
    "        else:\n",
    "            return f\"Table does not exist: {table_name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error querying table: {str(e)}\"\n",
    "\n",
    "filters = [\n",
    "    (\"event_date\", \"equals\", event_date),\n",
    "    (\"hr\", \"equals\", event_hour)\n",
    "]\n",
    "\n",
    "filtered_results = query_with_filters(catalog, iceberg_namespace, iceberg_table, filters)\n",
    "if isinstance(filtered_results, pd.DataFrame):\n",
    "    print(f\"Retrieved {len(filtered_results)} rows matching filters\")\n",
    "    filtered_results.head()\n",
    "else:\n",
    "    print(filtered_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
