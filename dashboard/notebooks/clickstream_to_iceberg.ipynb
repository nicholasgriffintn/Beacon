{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2 Clickstream Data to Iceberg\n",
    "\n",
    "This notebook loads clickstream data from R2 for a specific day and hour, then loads it into an Iceberg table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.11/site-packages (1.38.13)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.13 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.38.13)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /opt/conda/lib/python3.11/site-packages (from boto3) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.39.0,>=1.38.13->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.39.0,>=1.38.13->boto3) (2.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.13->boto3) (1.16.0)\n",
      "Requirement already satisfied: duckdb in /opt/conda/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: pyiceberg in /opt/conda/lib/python3.11/site-packages (0.9.1)\n",
      "Requirement already satisfied: cachetools<6.0.0,>=5.5.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (5.5.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (8.1.7)\n",
      "Requirement already satisfied: fsspec>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (2025.3.2)\n",
      "Requirement already satisfied: mmh3<6.0.0,>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (5.1.0)\n",
      "Requirement already satisfied: pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (2.11.4)\n",
      "Requirement already satisfied: pyparsing<4.0.0,>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (3.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.20.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (2.31.0)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (13.9.4)\n",
      "Requirement already satisfied: sortedcontainers==2.4.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (2.4.0)\n",
      "Requirement already satisfied: strictyaml<2.0.0,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (1.7.3)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=8.2.3 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (9.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0->pyiceberg) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.11/site-packages (from pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0->pyiceberg) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.11/site-packages (from pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0->pyiceberg) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0->pyiceberg) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.20.0->pyiceberg) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.20.0->pyiceberg) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.20.0->pyiceberg) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.20.0->pyiceberg) (2023.7.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->pyiceberg) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->pyiceberg) (2.16.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from strictyaml<2.0.0,>=1.7.0->pyiceberg) (2.8.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->pyiceberg) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.6.0->strictyaml<2.0.0,>=1.7.0->pyiceberg) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import sys\n",
    "import json\n",
    "# Install Boto3 if not already installed\n",
    "!{sys.executable} -m pip install boto3\n",
    "import boto3\n",
    "# Install duckdb if not already installed\n",
    "!{sys.executable} -m pip install duckdb\n",
    "import duckdb\n",
    "# Install PyIceberg if not already installed\n",
    "!{sys.executable} -m pip install pyiceberg\n",
    "from pyiceberg.catalog.rest import RestCatalog\n",
    "from pyiceberg.exceptions import NamespaceAlreadyExistsError\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.transforms import IdentityTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Environment\n",
    "\n",
    "Set up environment variables for both R2 and Iceberg catalog connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Bucket: analytics-pipeline\n",
      "Warehouse: 95afa6fa9aba1181119ed0aeeac7bfb8_analytics-pipeline\n",
      "Catalog URI: https://catalog.cloudflarestorage.com/95afa6fa9aba1181119ed0aeeac7bfb8/analytics-pipeline\n"
     ]
    }
   ],
   "source": [
    "r2_access_key = os.environ.get(\"R2_ACCESS_KEY_ID\")\n",
    "r2_secret_key = os.environ.get(\"R2_SECRET_ACCESS_KEY\")\n",
    "r2_endpoint = os.environ.get(\"R2_ENDPOINT\")\n",
    "r2_bucket = os.environ.get(\"R2_CLICKSTREAM_BUCKET\", \"analytics-pipeline\")\n",
    "\n",
    "warehouse = os.environ.get(\"WAREHOUSE\")\n",
    "token = os.environ.get(\"TOKEN\")\n",
    "catalog_uri = os.environ.get(\"CATALOG_URI\")\n",
    "\n",
    "missing_r2 = []\n",
    "if not r2_endpoint: missing_r2.append(\"R2_ENDPOINT\")\n",
    "if not r2_access_key: missing_r2.append(\"R2_ACCESS_KEY\")\n",
    "if not r2_secret_key: missing_r2.append(\"R2_SECRET_KEY\")\n",
    "\n",
    "missing_iceberg = []\n",
    "if not warehouse: missing_iceberg.append(\"WAREHOUSE\")\n",
    "if not token: missing_iceberg.append(\"TOKEN\")\n",
    "if not catalog_uri: missing_iceberg.append(\"CATALOG_URI\")\n",
    "\n",
    "if missing_r2:\n",
    "    print(f\"Warning: Missing required R2 credentials: {', '.join(missing_r2)}\")\n",
    "if missing_iceberg:\n",
    "    print(f\"Warning: Missing required Iceberg credentials: {', '.join(missing_iceberg)}\")\n",
    "\n",
    "print(f\"R2 Bucket: {r2_bucket}\")\n",
    "print(f\"Warehouse: {warehouse}\")\n",
    "print(f\"Catalog URI: {catalog_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to R2 and Iceberg\n",
    "\n",
    "Establish connections to R2 Storage and Iceberg catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to R2 successfully!\n",
      "Connected to Iceberg catalog successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if all([r2_access_key, r2_secret_key]):\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=r2_access_key,\n",
    "            aws_secret_access_key=r2_secret_key,\n",
    "            endpoint_url=r2_endpoint\n",
    "        )\n",
    "        print(\"Connected to R2 successfully!\")\n",
    "    else:\n",
    "        print(\"Cannot connect to R2 - missing required credentials\")\n",
    "        s3_client = None\n",
    "except Exception as e:\n",
    "    print(f\"R2 connection failed: {str(e)}\")\n",
    "    s3_client = None\n",
    "\n",
    "try:\n",
    "    if all([warehouse, token, catalog_uri]):\n",
    "        catalog = RestCatalog(\n",
    "            name=\"my_catalog\",\n",
    "            warehouse=warehouse,\n",
    "            uri=catalog_uri,\n",
    "            token=token,\n",
    "        )\n",
    "        print(\"Connected to Iceberg catalog successfully!\")\n",
    "    else:\n",
    "        print(\"Cannot connect to Iceberg catalog - missing required credentials\")\n",
    "        catalog = None\n",
    "except Exception as e:\n",
    "    print(f\"Iceberg connection failed: {str(e)}\")\n",
    "    catalog = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Parameters\n",
    "\n",
    "Configure which day and hour of data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process data in: event_date=2025-05-11/hr=01/\n"
     ]
    }
   ],
   "source": [
    "event_date = \"2025-05-11\"  # Format: YYYY-MM-DD\n",
    "event_hour = \"01\"          # Format: HH (24-hour)\n",
    "\n",
    "s3_prefix = f\"event_date={event_date}/hr={event_hour}/\"\n",
    "print(f\"Will process data in: {s3_prefix}\")\n",
    "\n",
    "iceberg_namespace = \"default\"\n",
    "iceberg_table = \"analytics_pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Namespace (if needed)\n",
    "\n",
    "Ensure the target namespace exists in the Iceberg catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'default' namespace already exists\n"
     ]
    }
   ],
   "source": [
    "if catalog is not None:\n",
    "    try:\n",
    "        catalog.create_namespace(iceberg_namespace)\n",
    "        print(f\"Created '{iceberg_namespace}' namespace\")\n",
    "    except NamespaceAlreadyExistsError:\n",
    "        print(f\"'{iceberg_namespace}' namespace already exists\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating namespace: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available S3 Objects\n",
    "\n",
    "Check what data files are available for the selected day and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 objects in the specified path\n",
      "- event_date=2025-05-11/hr=01/01JTYEACHNFW40C9T3NH61E48V.json.gz\n",
      "- event_date=2025-05-11/hr=01/01JTYEBTYZN12WQS5GB9X9FFQZ.json.gz\n",
      "- event_date=2025-05-11/hr=01/01JTYET1X4RTC6RZW95W9REVGY.json.gz\n",
      "- event_date=2025-05-11/hr=01/01JTYF21A1EYXCVTG9NZVQNAF3.json.gz\n",
      "- event_date=2025-05-11/hr=01/01JTYFF2RG7AQDGGD96WMSJGVZ.json.gz\n",
      "... and 4 more\n"
     ]
    }
   ],
   "source": [
    "def list_s3_objects(bucket, prefix):\n",
    "    if s3_client is None:\n",
    "        return \"Not connected to R2\"\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        if 'Contents' in response:\n",
    "            return [obj['Key'] for obj in response['Contents']]\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        return f\"Error listing objects: {str(e)}\"\n",
    "\n",
    "s3_objects = list_s3_objects(r2_bucket, s3_prefix)\n",
    "\n",
    "if isinstance(s3_objects, list):\n",
    "    print(f\"Found {len(s3_objects)} objects in the specified path\")\n",
    "    if s3_objects:\n",
    "        for i, obj in enumerate(s3_objects[:5]):\n",
    "            print(f\"- {obj}\")\n",
    "        if len(s3_objects) > 5:\n",
    "            print(f\"... and {len(s3_objects) - 5} more\")\n",
    "    else:\n",
    "        print(\"No data files found for the specified date and hour\")\n",
    "else:\n",
    "    print(s3_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from R2\n",
    "\n",
    "Load the clickstream data from R2 into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing event_date=2025-05-11/hr=01/01JTYEACHNFW40C9T3NH61E48V.json.gz with DuckDB...\n",
      "Reading s3://analytics-pipeline/event_date=2025-05-11/hr=01/01JTYEACHNFW40C9T3NH61E48V.json.gz\n",
      "Successfully loaded 17 rows from event_date=2025-05-11/hr=01/01JTYEACHNFW40C9T3NH61E48V.json.gz\n",
      "Processing event_date=2025-05-11/hr=01/01JTYEBTYZN12WQS5GB9X9FFQZ.json.gz with DuckDB...\n",
      "Reading s3://analytics-pipeline/event_date=2025-05-11/hr=01/01JTYEBTYZN12WQS5GB9X9FFQZ.json.gz\n",
      "Successfully loaded 4 rows from event_date=2025-05-11/hr=01/01JTYEBTYZN12WQS5GB9X9FFQZ.json.gz\n",
      "Processing event_date=2025-05-11/hr=01/01JTYET1X4RTC6RZW95W9REVGY.json.gz with DuckDB...\n",
      "Reading s3://analytics-pipeline/event_date=2025-05-11/hr=01/01JTYET1X4RTC6RZW95W9REVGY.json.gz\n",
      "Successfully loaded 12 rows from event_date=2025-05-11/hr=01/01JTYET1X4RTC6RZW95W9REVGY.json.gz\n",
      "Processing event_date=2025-05-11/hr=01/01JTYF21A1EYXCVTG9NZVQNAF3.json.gz with DuckDB...\n",
      "Reading s3://analytics-pipeline/event_date=2025-05-11/hr=01/01JTYF21A1EYXCVTG9NZVQNAF3.json.gz\n",
      "Successfully loaded 4 rows from event_date=2025-05-11/hr=01/01JTYF21A1EYXCVTG9NZVQNAF3.json.gz\n",
      "Processing event_date=2025-05-11/hr=01/01JTYFF2RG7AQDGGD96WMSJGVZ.json.gz with DuckDB...\n",
      "Reading s3://analytics-pipeline/event_date=2025-05-11/hr=01/01JTYFF2RG7AQDGGD96WMSJGVZ.json.gz\n",
      "Successfully loaded 6 rows from event_date=2025-05-11/hr=01/01JTYFF2RG7AQDGGD96WMSJGVZ.json.gz\n",
      "Processing event_date=2025-05-11/hr=01/01JTYG7YJRB25JKNDXBTAMC221.json.gz with DuckDB...\n",
      "Reading s3://analytics-pipeline/event_date=2025-05-11/hr=01/01JTYG7YJRB25JKNDXBTAMC221.json.gz\n",
      "Successfully loaded 1 rows from event_date=2025-05-11/hr=01/01JTYG7YJRB25JKNDXBTAMC221.json.gz\n",
      "Processing event_date=2025-05-11/hr=01/01JTYG8CN3B632QYRFDM2HFDFX.json.gz with DuckDB...\n",
      "Reading s3://analytics-pipeline/event_date=2025-05-11/hr=01/01JTYG8CN3B632QYRFDM2HFDFX.json.gz\n",
      "Successfully loaded 2 rows from event_date=2025-05-11/hr=01/01JTYG8CN3B632QYRFDM2HFDFX.json.gz\n",
      "Processing event_date=2025-05-11/hr=01/01JTYGRB619MHQ26008E4XZ5R5.json.gz with DuckDB...\n",
      "Reading s3://analytics-pipeline/event_date=2025-05-11/hr=01/01JTYGRB619MHQ26008E4XZ5R5.json.gz\n",
      "Successfully loaded 2 rows from event_date=2025-05-11/hr=01/01JTYGRB619MHQ26008E4XZ5R5.json.gz\n",
      "Processing event_date=2025-05-11/hr=01/01JTYGSYNJDATBDBSKBS9KYTKV.json.gz with DuckDB...\n",
      "Reading s3://analytics-pipeline/event_date=2025-05-11/hr=01/01JTYGSYNJDATBDBSKBS9KYTKV.json.gz\n",
      "Successfully loaded 4 rows from event_date=2025-05-11/hr=01/01JTYGSYNJDATBDBSKBS9KYTKV.json.gz\n",
      "Loaded 52 total rows from 9 files\n",
      "Failed to load 0 files\n",
      "                  timestamp  \\\n",
      "0  2025-05-11T01:10:57.348Z   \n",
      "1  2025-05-11T01:10:57.348Z   \n",
      "2  2025-05-11T01:10:57.348Z   \n",
      "3  2025-05-11T01:10:57.348Z   \n",
      "4  2025-05-11T01:10:57.347Z   \n",
      "\n",
      "                                        session_data  \\\n",
      "0  {'site_id': 'polychat-metrics', 'user_id': 'us...   \n",
      "1  {'site_id': 'polychat-metrics', 'user_id': 'us...   \n",
      "2  {'site_id': 'polychat-metrics', 'user_id': 'us...   \n",
      "3  {'site_id': 'polychat-metrics', 'user_id': 'us...   \n",
      "4  {'site_id': 'polychat-metrics', 'user_id': 'us...   \n",
      "\n",
      "                                         device_info referrer            ip  \\\n",
      "0  {'browser': 'Unknown', 'os': 'Unknown', 'devic...       NA  80.5.132.215   \n",
      "1  {'browser': 'Unknown', 'os': 'Unknown', 'devic...       NA  80.5.132.215   \n",
      "2  {'browser': 'Unknown', 'os': 'Unknown', 'devic...       NA  80.5.132.215   \n",
      "3  {'browser': 'Unknown', 'os': 'Unknown', 'devic...       NA  80.5.132.215   \n",
      "4  {'browser': 'Unknown', 'os': 'Unknown', 'devic...       NA  80.5.132.215   \n",
      "\n",
      "                  event_data  \\\n",
      "0  {'event_type': 'unknown'}   \n",
      "1  {'event_type': 'unknown'}   \n",
      "2  {'event_type': 'unknown'}   \n",
      "3  {'event_type': 'unknown'}   \n",
      "4  {'event_type': 'unknown'}   \n",
      "\n",
      "                                           raw_event data_type  \\\n",
      "0  {'s': 'polychat-metrics', 'ts': '1746925852145...   unknown   \n",
      "1  {'s': 'polychat-metrics', 'ts': '1746925852156...   unknown   \n",
      "2  {'s': 'polychat-metrics', 'ts': '1746925852156...   unknown   \n",
      "3  {'s': 'polychat-metrics', 'ts': '1746925852156...   unknown   \n",
      "4  {'s': 'polychat-metrics', 'ts': '1746925852145...   unknown   \n",
      "\n",
      "                                            filename event_date  hr  \n",
      "0  s3://analytics-pipeline/event_date=2025-05-11/... 2025-05-11  01  \n",
      "1  s3://analytics-pipeline/event_date=2025-05-11/... 2025-05-11  01  \n",
      "2  s3://analytics-pipeline/event_date=2025-05-11/... 2025-05-11  01  \n",
      "3  s3://analytics-pipeline/event_date=2025-05-11/... 2025-05-11  01  \n",
      "4  s3://analytics-pipeline/event_date=2025-05-11/... 2025-05-11  01  \n"
     ]
    }
   ],
   "source": [
    "db_file = \"/home/jovyan/duckdb_data/clickstream.db\"\n",
    "os.makedirs(os.path.dirname(db_file), exist_ok=True)\n",
    "\n",
    "con = duckdb.connect(db_file)\n",
    "\n",
    "# Optional: Set some pragmas for better performance\n",
    "con.execute(\"PRAGMA temp_directory='/home/jovyan/duckdb_data/temp'\")\n",
    "con.execute(\"PRAGMA memory_limit='100GB'\")\n",
    "\n",
    "def read_s3_with_duckdb(bucket, key):\n",
    "    try:\n",
    "        con.execute(f\"\"\"\n",
    "            SET s3_region='auto';\n",
    "            SET s3_endpoint='{r2_endpoint.replace('https://', '')}';\n",
    "            SET s3_access_key_id='{r2_access_key}';\n",
    "            SET s3_secret_access_key='{r2_secret_key}';\n",
    "        \"\"\")\n",
    "        \n",
    "        s3_url = f\"s3://{bucket}/{key}\"\n",
    "        print(f\"Reading {s3_url}\")\n",
    "        \n",
    "        if key.endswith('.json.gz'):\n",
    "            query = f\"\"\"\n",
    "                SELECT * FROM read_json_auto('{s3_url}', \n",
    "                    filename=true, \n",
    "                    maximum_object_size=3000000000,\n",
    "                    ignore_errors=true)\n",
    "            \"\"\"\n",
    "        elif key.endswith('.parquet'):\n",
    "            query = f\"SELECT * FROM read_parquet('{s3_url}')\"\n",
    "        elif key.endswith('.csv') or key.endswith('.tsv') or key.endswith('.txt'):\n",
    "            query = f\"SELECT * FROM read_csv_auto('{s3_url}')\"\n",
    "        else:\n",
    "            return None, f\"Unsupported file type: {key}\"\n",
    "        \n",
    "        result = con.execute(query).fetchdf()\n",
    "        return result, None\n",
    "    except Exception as e:\n",
    "        return None, f\"Error reading file with DuckDB: {str(e)}\"\n",
    "\n",
    "all_data = []\n",
    "error_count = 0\n",
    "valid_count = 0\n",
    "\n",
    "if isinstance(s3_objects, list) and s3_objects:\n",
    "    for obj_key in s3_objects:\n",
    "        print(f\"Processing {obj_key} with DuckDB...\")\n",
    "        \n",
    "        df, error = read_s3_with_duckdb(r2_bucket, obj_key)\n",
    "            \n",
    "        if df is not None:\n",
    "            row_count = len(df)\n",
    "            print(f\"Successfully loaded {row_count} rows from {obj_key}\")\n",
    "            all_data.append(df)\n",
    "            valid_count += 1\n",
    "        else:\n",
    "            print(f\"Failed to load {obj_key}: {error}\")\n",
    "            error_count += 1\n",
    "    \n",
    "    if all_data:\n",
    "        clickstream_data = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"Loaded {len(clickstream_data)} total rows from {valid_count} files\")\n",
    "        print(f\"Failed to load {error_count} files\")\n",
    "        \n",
    "        print(clickstream_data.head())\n",
    "    else:\n",
    "        print(\"No data was loaded successfully\")\n",
    "        clickstream_data = None\n",
    "else:\n",
    "    print(\"No data files to process\")\n",
    "    clickstream_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data for Iceberg\n",
    "\n",
    "Clean and prepare the data for Iceberg storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed successfully: 52 rows\n"
     ]
    }
   ],
   "source": [
    "def process_clickstream_data(df):\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    if 'timestamp' in processed_df.columns and processed_df['timestamp'].dtype == 'object':\n",
    "        processed_df['timestamp'] = pd.to_datetime(processed_df['timestamp'])\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "if clickstream_data is not None:\n",
    "    processed_data = process_clickstream_data(clickstream_data)\n",
    "    if processed_data is not None:\n",
    "        print(f\"Data processed successfully: {len(processed_data)} rows\")\n",
    "        processed_data.head()\n",
    "    else:\n",
    "        print(\"Data processing failed\")\n",
    "else:\n",
    "    processed_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Iceberg Table\n",
    "\n",
    "Create a new Iceberg table if it doesn't exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing nested objects in column session_data\n",
      "Processing nested objects in column device_info\n",
      "Processing nested objects in column event_data\n",
      "Processing nested objects in column raw_event\n",
      "Converting timestamp to ISO format string\n",
      "Converting event_date datetime to string format\n",
      "Converting column session_data to JSON strings\n",
      "Converting column device_info to JSON strings\n",
      "Converting column event_data to JSON strings\n",
      "Converting column raw_event to JSON strings\n",
      "Data converted to PyArrow Table successfully\n",
      "Schema: timestamp: string\n",
      "session_data: string\n",
      "device_info: string\n",
      "referrer: string\n",
      "ip: string\n",
      "event_data: string\n",
      "raw_event: string\n",
      "data_type: string\n",
      "filename: string\n",
      "event_date: string\n",
      "hr: string\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 1355\n",
      "Created table: analytics_pipeline\n"
     ]
    }
   ],
   "source": [
    "def df_to_pyarrow(df):\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        for col in df_copy.select_dtypes(include=[\"object\"]):\n",
    "            if df_copy[col].dropna().apply(lambda x: isinstance(x, uuid.UUID)).any():\n",
    "                print(f\"Converting UUID column {col} to strings\")\n",
    "                df_copy[col] = df_copy[col].astype(str)\n",
    "        \n",
    "        def stringify_uuids(obj):\n",
    "            if isinstance(obj, uuid.UUID):\n",
    "                return str(obj)\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: stringify_uuids(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [stringify_uuids(v) for v in obj]\n",
    "            return obj\n",
    "        \n",
    "        for col in ['session_data', 'device_info', 'event_data', 'raw_event']:\n",
    "            if col in df_copy.columns:\n",
    "                print(f\"Processing nested objects in column {col}\")\n",
    "                df_copy[col] = df_copy[col].apply(\n",
    "                    lambda x: stringify_uuids(x) if pd.notna(x) else x\n",
    "                )\n",
    "        \n",
    "        if 'timestamp' in df_copy.columns and pd.api.types.is_datetime64_any_dtype(df_copy['timestamp']):\n",
    "            print(\"Converting timestamp to ISO format string\")\n",
    "            df_copy['timestamp'] = df_copy['timestamp'].apply(\n",
    "                lambda x: x.isoformat() if pd.notna(x) else None\n",
    "            )\n",
    "        \n",
    "        if 'event_date' in df_copy.columns:\n",
    "            if pd.api.types.is_datetime64_any_dtype(df_copy['event_date']):\n",
    "                print(\"Converting event_date datetime to string format\")\n",
    "                df_copy['event_date'] = df_copy['event_date'].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        for col in ['session_data', 'device_info', 'event_data', 'raw_event']:\n",
    "            if col in df_copy.columns:\n",
    "                print(f\"Converting column {col} to JSON strings\")\n",
    "                df_copy[col] = df_copy[col].apply(\n",
    "                    lambda x: json.dumps(x) if isinstance(x, (dict, list)) else \n",
    "                             (str(x) if not isinstance(x, (str, int, float, bool, type(None))) else x)\n",
    "                )\n",
    "        \n",
    "        arrow_table = pa.Table.from_pandas(df_copy, preserve_index=False)\n",
    "        return arrow_table\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting to PyArrow: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def create_iceberg_table(catalog, namespace, table_name, schema):\n",
    "    if catalog is None:\n",
    "        return \"Not connected to Iceberg catalog\"\n",
    "    \n",
    "    try:\n",
    "        table_identifier = (namespace, table_name)\n",
    "        if not catalog.table_exists(table_identifier):\n",
    "            table = catalog.create_table(\n",
    "                table_identifier,\n",
    "                schema=schema\n",
    "            )\n",
    "            return f\"Created table: {table_name}\"\n",
    "        else:\n",
    "            table = catalog.load_table(table_identifier)\n",
    "            return f\"Table already exists: {table_name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error creating/loading table: {str(e)}\"\n",
    "\n",
    "if processed_data is not None:\n",
    "    arrow_table = df_to_pyarrow(processed_data)\n",
    "    if arrow_table is not None:\n",
    "        print(\"Data converted to PyArrow Table successfully\")\n",
    "        print(f\"Schema: {arrow_table.schema}\")\n",
    "        \n",
    "        table_result = create_iceberg_table(catalog, iceberg_namespace, iceberg_table, arrow_table.schema)\n",
    "        print(table_result)\n",
    "    else:\n",
    "        print(\"Failed to convert data to PyArrow Table\")\n",
    "else:\n",
    "    print(\"No processed data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data to Iceberg Table\n",
    "\n",
    "Write the processed clickstream data to the Iceberg table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended to table: analytics_pipeline\n"
     ]
    }
   ],
   "source": [
    "def append_to_iceberg_table(catalog, namespace, table_name, data):\n",
    "    if catalog is None:\n",
    "        return \"Not connected to Iceberg catalog\"\n",
    "    \n",
    "    if data is None:\n",
    "        return \"No data to append\"\n",
    "        \n",
    "    try:\n",
    "        table_identifier = (namespace, table_name)\n",
    "        if catalog.table_exists(table_identifier):\n",
    "            table = catalog.load_table(table_identifier)\n",
    "            \n",
    "            table.append(data)\n",
    "            return f\"Data appended to table: {table_name}\"\n",
    "        else:\n",
    "            return f\"Table does not exist: {table_name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error appending data: {str(e)}\"\n",
    "\n",
    "if arrow_table is not None:\n",
    "    append_result = append_to_iceberg_table(catalog, iceberg_namespace, iceberg_table, arrow_table)\n",
    "    print(append_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data in Iceberg Table\n",
    "\n",
    "Query the Iceberg table to verify the data was written successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 52 rows from Iceberg table\n"
     ]
    }
   ],
   "source": [
    "def query_iceberg_table(catalog, namespace, table_name):\n",
    "    if catalog is None:\n",
    "        return \"Not connected to Iceberg catalog\"\n",
    "        \n",
    "    try:\n",
    "        table_identifier = (namespace, table_name)\n",
    "        if catalog.table_exists(table_identifier):\n",
    "            table = catalog.load_table(table_identifier)\n",
    "            \n",
    "            # Get the scan results\n",
    "            scan = table.scan()\n",
    "            \n",
    "            # Process each task separately and combine results\n",
    "            all_tables = []\n",
    "            for task in scan.plan_files():\n",
    "                # Get the S3 path\n",
    "                s3_path = task.file.file_path\n",
    "                # Extract bucket and key from the path\n",
    "                bucket = s3_path.split('/')[2]\n",
    "                key = '/'.join(s3_path.split('/')[3:])\n",
    "                \n",
    "                # Download the parquet file to a temporary location\n",
    "                import tempfile\n",
    "                with tempfile.NamedTemporaryFile(suffix='.parquet') as tmp:\n",
    "                    s3_client.download_file(bucket, key, tmp.name)\n",
    "                    df = pd.read_parquet(tmp.name)\n",
    "                    all_tables.append(df)\n",
    "            \n",
    "            if all_tables:\n",
    "                # Combine all DataFrames\n",
    "                result = pd.concat(all_tables, ignore_index=True)\n",
    "                return result\n",
    "            else:\n",
    "                return \"Table exists but has no data\"\n",
    "        else:\n",
    "            return f\"Table does not exist: {table_name}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying table: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"Error querying table: {str(e)}\"\n",
    "\n",
    "result = query_iceberg_table(catalog, iceberg_namespace, iceberg_table)\n",
    "if isinstance(result, pd.DataFrame):\n",
    "    print(f\"Retrieved {len(result)} rows from Iceberg table\")\n",
    "    result.head()\n",
    "else:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data by Partition\n",
    "\n",
    "Query the Iceberg table with partition filters to verify partitioning works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 52 rows matching filters\n"
     ]
    }
   ],
   "source": [
    "def query_with_filters(catalog, namespace, table_name, filters):\n",
    "    if catalog is None:\n",
    "        return \"Not connected to Iceberg catalog\"\n",
    "        \n",
    "    try:\n",
    "        table_identifier = (namespace, table_name)\n",
    "        if catalog.table_exists(table_identifier):\n",
    "            table = catalog.load_table(table_identifier)\n",
    "            \n",
    "            # Create a scan with filters\n",
    "            scan = table.scan()\n",
    "            for col, op, val in filters:\n",
    "                if op == \"equals\":\n",
    "                    scan = scan.filter(f\"{col} = '{val}'\")\n",
    "            \n",
    "            # Get results by reading each file separately\n",
    "            all_tables = []\n",
    "            for task in scan.plan_files():\n",
    "                s3_path = task.file.file_path\n",
    "                bucket = s3_path.split('/')[2]\n",
    "                key = '/'.join(s3_path.split('/')[3:])\n",
    "                \n",
    "                import tempfile\n",
    "                with tempfile.NamedTemporaryFile(suffix='.parquet') as tmp:\n",
    "                    s3_client.download_file(bucket, key, tmp.name)\n",
    "                    df = pd.read_parquet(tmp.name)\n",
    "                    all_tables.append(df)\n",
    "            \n",
    "            if all_tables:\n",
    "                result = pd.concat(all_tables, ignore_index=True)\n",
    "                return result\n",
    "            else:\n",
    "                return \"No data matching filters\"\n",
    "        else:\n",
    "            return f\"Table does not exist: {table_name}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying table: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"Error querying table: {str(e)}\"\n",
    "\n",
    "filters = [\n",
    "    (\"event_date\", \"equals\", event_date),\n",
    "    (\"hr\", \"equals\", event_hour)\n",
    "]\n",
    "\n",
    "filtered_results = query_with_filters(catalog, iceberg_namespace, iceberg_table, filters)\n",
    "if isinstance(filtered_results, pd.DataFrame):\n",
    "    print(f\"Retrieved {len(filtered_results)} rows matching filters\")\n",
    "    filtered_results.head()\n",
    "else:\n",
    "    print(filtered_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
